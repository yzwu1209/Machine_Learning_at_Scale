{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* algorithm using Spark\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2019`__\n",
    "\n",
    "## VERY IMPORTANT NOTE: \n",
    "Unlike the regular assignments, this assignment is much more open ended. It is up to you to think through the problem, and make decisions about what is applicable, and what intermediate tasks are suitable to solve the problem. There are 4 tasks listed below, but these are only a guide. Your grade will be based on the quality of your approach, your reasoning, etc., and not as much on the accuracy of your results. So please provide justification for your design choices. And as always, have fun!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dijsktras video](https://www.youtube.com/watch?v=pVfj6mxhdMw)   \n",
    "[A* video](https://www.youtube.com/watch?v=eSOJ3ARN5FM&feature=youtu.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our distributed SSSP algorithm, in order to find the shortest path, we must visit all nodes in the graph. This is also true of the single core Dijkstras algorithm. On a very large graph, this can be very resource intensive. A * addresses this problem by assigning a heuristic to each node which is a 'best guess' as to the distance from that node to the destination. This hueristic is used to prioritize the path taken to the target node, and the algorithm terminates when the target node is reached. This means that not all nodes must neccessarily be visited. The key to A * is to choose a good heuristic. If we underestimate the distance, the algorithm may end up visiting all nodes after all. If we overestimate the distance, we could end up with a poor solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2GB Wikipedia dataset:\n",
    "https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0.\n",
    "\n",
    "## DATA Format:   \n",
    "`node_id \\t {neighbor_id:count, neighbor_id:count,...}`    \n",
    "Where the count is the number of times the link appears on the page. We'll treat this as the \"weight\" of the edge between pages. \n",
    "\n",
    "The challenge in this case, is to find a good heuristic \"distance\" metric between pages. What does distance mean in this setting, where weights are the number of links? Is there a trade-off in calculating a \"good\" heuristic vs visiting all nodes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "* Come up with a good representative toy example. \n",
    "* Come up with your heuristic.\n",
    "* Hand calculate A*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Come up with a good representative toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"Astar_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{'2': 1, '3': 2}\n",
      "2\t{'3': 2, '4': 3, '5': 5}\n",
      "3\t{'4': 2}\n",
      "4\t{'5': 3}\n"
     ]
    }
   ],
   "source": [
    "# 1. Toy example - weighted directed toy example\n",
    "!cat data/directed_toy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDD = sc.textFile('data/directed_toy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Come up with your heuristic\n",
    "\n",
    "> Heruistic distances of each node to the end node are marked down in **red**, whereas the weights on each edge are marked down in **blue**. The image is first drawn on grids. Using the coordinates of each node, the heuristic distance is calculated based on Manhattan Distance. \n",
    "\n",
    "> <img src=\"Toy_Example2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Hand calculate A*\n",
    "\n",
    "- Starting with Node 1: $f = g + h = 0 + 8 = 8$\n",
    "- Node 1 goes to Node 2 and Node 3. \n",
    "    - Node 2: $f = g + h = 1 + 7 = 8 \\text{ (Node 1)}$ \n",
    "    - Node 3: $f = g + h = 2 + 9 = 11 \\text{ (Node 1)}$\n",
    "- Since $8<11$, next we expand to Node 2, which is connected to Node 3, 4, and 5.\n",
    "    - Node 3: $f = g + h = 3 + 9 = 12 \\text{ (Node 2)}$\n",
    "    - Node 4: $f = g + h = 4 + 5 = 9 \\text{ (Node 2)}$\n",
    "    - Node 5: $f = g + h = 6 + 0 = 6 \\text{ (Node 2)}$\n",
    "- Since $6$ is the smallest, next we expand to Node 5. Notice that Node 5 is our end node. Our calculation stops here. \n",
    "- Therefore, the shortest path from Node 1 to 5 is: Node 1 - 2 - 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Implement Parallel A*. You can use the provided code as a strating point, but you don't have to. You are free to use RDDs, DataFrames, or GraphFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "# Spark only implements Accumulator parameter for numeric types.\n",
    "# This class extends Accumulator support to the string type.\n",
    "class StringAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 + val2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Run your implementation on your toy examples. Note: You may want to come up with several toy examples to test how your algorithm performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Finally, run your algorithm on the full dataset and compare your results to the SSSP implementation in terms of runtime performance, as well as accuracy. Discuss your findings, tradeoffs, challenges, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone SSSP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sssp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sssp.py\n",
    "from __future__ import print_function\n",
    "import ast\n",
    "import sys\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark only implements Accumulator parameter for numeric types.\n",
    "# This class extends Accumulator support to the string type.\n",
    "class StringAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 + val2\n",
    "\n",
    "\n",
    "###################################\n",
    "# INITIALIZE GRAPH\n",
    "###################################\n",
    "\n",
    "def parseData(line):\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    key = str(key)\n",
    "  \n",
    "    if key == startNode.value:\n",
    "        return (key, (\"Q\",ast.literal_eval(value),0,key))\n",
    "    else:\n",
    "        return (key, (\"U\",ast.literal_eval(value),float(\"inf\"),\"\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "###################################\n",
    "# MAPPER\n",
    "###################################   \n",
    "\n",
    "def expandFrontier(row):\n",
    "    key = row[0]\n",
    "    status = row[1][0]\n",
    "    neighbors = row[1][1]\n",
    "    distance = row[1][2]\n",
    "    path = row[1][3]\n",
    "  \n",
    "    if status == \"Q\":\n",
    "    \n",
    "    # put neighbors in Q mode and update path length by incrementing path length of N\n",
    "        for neighbor in neighbors:\n",
    "            yield neighbor, (\"Q\", {}, distance + int(neighbors[neighbor]), str(path)+\" -> \"+str(neighbor))\n",
    "      \n",
    "    # Update status of current node to Visited\n",
    "    status = \"V\"\n",
    "      \n",
    "    yield key, (status, neighbors, distance, path)\n",
    "\n",
    "\n",
    "###################################\n",
    "# REDUCER\n",
    "###################################\n",
    "\n",
    "def restoreGraph(a,b):\n",
    "    \n",
    "    # It's important that the node in status Q comes first.\n",
    "    a,b = sorted([a,b]) \n",
    "    \n",
    "    _status, _neighbors, _distance, _path = a # <- Q state (if there is a Q state)\n",
    "    status, neighbors, distance, path = b # <- V or U state\n",
    "\n",
    "    if distance > _distance: # if the new path we discovered is shorter than the distance in a visited node, reset the visited node to Q state\n",
    "        status = \"Q\" # <- the magic for weighted graphs\n",
    "        distance = _distance\n",
    "        path = _path            \n",
    "\n",
    "    return (status, neighbors, distance, path)  \n",
    "\n",
    "  \n",
    "###################################\n",
    "# ACCUMULATORS\n",
    "###################################  \n",
    "  \n",
    "def terminate(row):\n",
    "    if row[1][0] == \"V\" and row[0] == targetNode.value:  \n",
    "        targetAccum.add(1)\n",
    "        pathAccum.add(str(row[1][3])+\" distance: \"+str(row[1][2]))\n",
    "    if row[1][0] == \"Q\":\n",
    "        statusAccum.add(1)\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    if len(sys.argv) != 5:\n",
    "        print(\"Usage: SSSP <file> <startNode> <targetNode> <isWeighted: 0|1>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    \n",
    "    app_name = \"graphs-intro\"\n",
    "    master = \"local[*]\"\n",
    "  \n",
    "    spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .appName(app_name) \\\n",
    "          .master(master) \\\n",
    "          .getOrCreate()\n",
    "  \n",
    "    sc = spark.sparkContext\n",
    "  \n",
    "    # remember to broadcast global variables:\n",
    "    dataFile = sc.textFile(sys.argv[1])\n",
    "    startNode = sc.broadcast(sys.argv[2])\n",
    "    targetNode = sc.broadcast(sys.argv[3])\n",
    "    weighted = sys.argv[4]\n",
    "  \n",
    "    rdd = dataFile.map(parseData).cache()\n",
    "\n",
    "    notconverged = True\n",
    "    iteration = 0\n",
    "    while notconverged:\n",
    "        iteration = iteration + 1\n",
    "        targetAccum = sc.accumulator(0)\n",
    "        statusAccum = sc.accumulator(0)\n",
    "        pathAccum = sc.accumulator(\"\", StringAccumulatorParam())\n",
    "\n",
    "        rdd = rdd.flatMap(expandFrontier).reduceByKey(restoreGraph)\n",
    "\n",
    "        rdd.foreach(terminate)\n",
    "\n",
    "        if weighted == \"1\":\n",
    "            if statusAccum.value == 0: # no more nodes in Q status\n",
    "            notconverged = False\n",
    "        else:\n",
    "            if targetAccum.value == 1: # reached target node\n",
    "            notconverged = False\n",
    "\n",
    "        print(\"-\"*50)  \n",
    "        print (\"After Iteration \"+str(iteration))\n",
    "        print(\"Node id, (Status, {out_nodes},distance,path)\")\n",
    "\n",
    "        for i in rdd.collect():\n",
    "            print(i)\n",
    "\n",
    "        print(\"Num nodes in Q status: \",statusAccum.value)\n",
    "        #print(\"Target node in V status: \",targetAccum.value)  # we only care about this in unweighted graphs, where reaching target node terminates the algorithim\n",
    "        print(\"-\"*50)    \n",
    "    \n",
    "\n",
    "    print(\"Num nodes in Q status: \",statusAccum.value)\n",
    "    #print(\"Target node in V status: \",targetAccum.value)\n",
    "    print(\"Iterations: \", iteration)\n",
    "    print(\"Path: \",pathAccum.value)\n",
    "    print(\"=\"*20)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "Alternative Search algorithms\n",
    "* Ripple Search [Brand et al., 2012]\n",
    "* I Fringe Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
