{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5Q8 - How to run in GCP\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "This notebook contains supplemental materials to help you run your HW5 solution to question 8 using Google Compute Platform. __Important Note:__ _the graders will not read this notebook. If you do use it, please be sure to copy relevant output back into the main homework notebook to receive credit for your results._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account setup\n",
    "1. Create your GCP account & apply for credit through the w261 education grant. (see [create_account.md](https://github.com/UCB-w261/w261-environment/blob/master/gcp/account-setup/create_account.md))\n",
    "2. Set up your project, bucket, service account, access key and virtual environment. (steps 1-15 in [setup.md](https://github.com/UCB-w261/w261-environment/blob/master/gcp/account-setup/setup.md))\n",
    "3. (OPTIONAL) Review the GCP documentation to become more familiar with the setup steps you've just performed: [key terms & concepts described here](https://cloud.google.com/storage/docs/concepts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the submission Script\n",
    "Copy the [submit_job_to_cluster.py](https://github.com/UCB-w261/w261-environment/blob/master/gcp/dataproc/submit_job_to_cluster.py) file from the environment repo into your current working directory. This script will help you run your own spark jobs on the cluster. You can read more about it here: [w261-environment](https://github.com/UCB-w261/w261-environment/tree/master/gcp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure to give your script executable permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x submit_job_to_cluster.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the data to your gcp bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To copy files from dropbox to google storage run:   \n",
    "`curl -L file-url | gsutil cp - gs://bucket-name/filename.txt`   \n",
    "\n",
    "For example, to stream the whole wiki graph from dropbox into my bucket, I would run:   \n",
    "`curl -L \"https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAD7I_6kQlJtDpXZPhCfVH-a/wikipedia/all-pages-indexed-out.txt?dl=0\" | gsutil cp - gs://w261-bucket/wiki_graph.txt`\n",
    "\n",
    "* To copy files from your computer to google storage, run:   \n",
    "`gsutil cp 'data/test_graph.txt' gs://bucket-name/test_graph.txt`   \n",
    "\n",
    "__IMPORTANT:__ You will need to run this outside of the Docker container, as the container doesn't have `gsutil` installed.\n",
    "\n",
    "For additonal information about moving files to your GS bucket, see: https://www.cloudbooklet.com/gsutil-cp-copy-and-move-files-on-google-cloud/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run a spark job on a cluster using GCP\n",
    "Fill in your PageRank code, and run the cell below to create a file called `pagerank.py` in the current directory.    \n",
    "__IMPORTANT:__ Make sure and fill in your own Bucket Name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile pagerank.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "\n",
    "############## YOUR BUCKET HERE ###############\n",
    "\n",
    "BUCKET=\"\"\n",
    "\n",
    "############## (END) YOUR BUCKET ###############\n",
    "\n",
    "\n",
    "wikiRDD = sc.textFile(\"gs://\"+BUCKET+\"/wiki_graph.txt\")\n",
    "\n",
    "\n",
    "def initGraph(dataRDD):\n",
    "    \"\"\"\n",
    "    Spark job to read in the raw data and initialize an \n",
    "    adjacency list representation with a record for each\n",
    "    node (including dangling nodes).\n",
    "    \n",
    "    Returns: \n",
    "        graphRDD -  a pair RDD of (node_id , (score, edges))\n",
    "        \n",
    "    NOTE: The score should be a float, but you may want to be \n",
    "    strategic about how format the edges... there are a few \n",
    "    options that can work. Make sure that whatever you choose\n",
    "    is sufficient for Question 8 where you'll run PageRank.\n",
    "    \"\"\"\n",
    "    ############## YOUR CODE HERE ###############\n",
    "    \n",
    "   \n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return graphRDD\n",
    "\n",
    "class FloatAccumulatorParam(AccumulatorParam):\n",
    "    \"\"\"\n",
    "    Custom accumulator for use in page rank to keep track of various masses.\n",
    "    \n",
    "    IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n",
    "    We stringly recommend you use the 'foreach' action in your implementation below.\n",
    "    \"\"\"\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 + val2\n",
    "    \n",
    "def runPageRank(graphInitRDD, alpha = 0.15, maxIter = 10, verbose = True):\n",
    "    \"\"\"\n",
    "    Spark job to implement page rank\n",
    "    Args: \n",
    "        graphInitRDD  - pair RDD of (node_id , (score, edges))\n",
    "        alpha         - (float) teleportation factor\n",
    "        maxIter       - (int) stopping criteria (number of iterations)\n",
    "        verbose       - (bool) option to print logging info after each iteration\n",
    "    Returns:\n",
    "        steadyStateRDD - pair RDD of (node_id, pageRank)\n",
    "    \"\"\"\n",
    "    # teleportation:\n",
    "    a = sc.broadcast(alpha)\n",
    "    \n",
    "    # damping factor:\n",
    "    d = sc.broadcast(1-a.value)\n",
    "    \n",
    "    # initialize accumulators for dangling mass & total mass\n",
    "    mmAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    \n",
    "    ############## YOUR CODE HERE ###############\n",
    "    \n",
    "    # write your helper functions here, \n",
    "    # please document the purpose of each clearly \n",
    "    # for reference, the master solution has 5 helper functions.\n",
    "    \n",
    "   \n",
    "\n",
    "               \n",
    "    # write your main Spark Job here (including the for loop to iterate)\n",
    "    # for reference, the master solution is 21 lines including comments & whitespace\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return steadyStateRDD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nIter = 10\n",
    "start = time.time()\n",
    "\n",
    "# Initialize your graph structure (Q7)\n",
    "wikiGraphRDD = initGraph(wikiRDD)\n",
    "\n",
    "# Run PageRank (Q8)\n",
    "full_results = runPageRank(wikiGraphRDD, alpha = 0.15, maxIter = nIter, verbose = True)\n",
    "\n",
    "print(f'...trained {nIter} iterations in {time.time() - start} seconds.')\n",
    "print(f'Top 20 ranked nodes:')\n",
    "print(full_results.takeOrdered(20, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this command in your terminal (Not in the Docker container!), to submit your job to GCP. You will need to have your environment variables pre-defined. Alterantively, substitute them with the actual values.   \n",
    "\n",
    "* PROJECT_ID: your GCP project id   \n",
    "* BUCKET_NAME: the name of your GCP bucket   \n",
    "* CLUSTER_NAME: choose a cluster name, this should include only a-z, 0-9 & start with a letter   \n",
    "* ZONE: The zone for your account and bucket, ex: us-central1-b\n",
    "\n",
    "\n",
    "```\n",
    "python3 submit_job_to_cluster.py \\\n",
    "    --project_id=${PROJECT_ID} \\\n",
    "    --zone=${ZONE} \\\n",
    "    --cluster_name=${CLUSTER_NAME} \\\n",
    "    --gcs_bucket=${BUCKET_NAME} \\\n",
    "    --key_file=$HOME/w261.json \\\n",
    "    --create_new_cluster \\\n",
    "    --pyspark_file=pagerank.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
